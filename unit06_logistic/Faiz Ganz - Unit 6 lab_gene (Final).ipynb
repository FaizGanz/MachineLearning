{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Faiz_Ganz_lab6_gene_partial.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Z76W9t8ZObp"
      },
      "source": [
        "# Lab:  Logistic Regression for Gene Expression Data\n",
        "\n",
        "In this lab, we use logistic regression to predict biological characteristics (\"phenotypes\") from gene expression data.  In addition to the concepts in [breast cancer demo](./breast_cancer.ipynb), you will learn to:\n",
        "* Handle missing data\n",
        "* Perform multi-class logistic classification\n",
        "* Create a confusion matrix\n",
        "* Use L1-regularization for improved estimation in the case of sparse weights (Grad students only)\n",
        "\n",
        "## Background\n",
        "\n",
        "Genes are the basic unit in the DNA and encode blueprints for proteins.  When proteins are synthesized from a gene, the gene is said to \"express\".  Micro-arrays are devices that measure the expression levels of large numbers of genes in parallel.  By finding correlations between expression levels and phenotypes, scientists can identify possible genetic markers for biological characteristics.\n",
        "\n",
        "The data in this lab comes from:\n",
        "\n",
        "https://archive.ics.uci.edu/ml/datasets/Mice+Protein+Expression\n",
        "\n",
        "In this data, mice were characterized by three properties:\n",
        "* Whether they had down's syndrome (trisomy) or not\n",
        "* Whether they were stimulated to learn or not\n",
        "* Whether they had a drug memantine or a saline control solution.\n",
        "\n",
        "With these three choices, there are 8 possible classes for each mouse.  For each mouse, the expression levels were measured across 77 genes.  We will see if the characteristics can be predicted from the gene expression levels.  This classification could reveal which genes are potentially involved in Down's syndrome and if drugs and learning have any noticeable effects.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULtPSZXcZObs"
      },
      "source": [
        "## Load the Data\n",
        "\n",
        "We begin by loading the standard modules."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxVAnNSMZObt"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from sklearn import linear_model, preprocessing"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n41xAmG6ZObu"
      },
      "source": [
        "Use the `pd.read_excel` command to read the data from \n",
        "\n",
        "https://archive.ics.uci.edu/ml/machine-learning-databases/00342/Data_Cortex_Nuclear.xls\n",
        "\n",
        "into a dataframe `df`.  Use the `index_col` option to specify that column 0 is the index.  Use the `df.head()` to print the first few rows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        },
        "id": "Ua8OfZTkZObu",
        "outputId": "4653a24c-875b-4096-df9f-39291749c511"
      },
      "source": [
        "# TODO 1\n",
        "df = pd.read_excel('https://archive.ics.uci.edu/ml/machine-learning-databases/00342/Data_Cortex_Nuclear.xls', index_col=0)\n",
        "df"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>DYRK1A_N</th>\n",
              "      <th>ITSN1_N</th>\n",
              "      <th>BDNF_N</th>\n",
              "      <th>NR1_N</th>\n",
              "      <th>NR2A_N</th>\n",
              "      <th>pAKT_N</th>\n",
              "      <th>pBRAF_N</th>\n",
              "      <th>pCAMKII_N</th>\n",
              "      <th>pCREB_N</th>\n",
              "      <th>pELK_N</th>\n",
              "      <th>pERK_N</th>\n",
              "      <th>pJNK_N</th>\n",
              "      <th>PKCA_N</th>\n",
              "      <th>pMEK_N</th>\n",
              "      <th>pNR1_N</th>\n",
              "      <th>pNR2A_N</th>\n",
              "      <th>pNR2B_N</th>\n",
              "      <th>pPKCAB_N</th>\n",
              "      <th>pRSK_N</th>\n",
              "      <th>AKT_N</th>\n",
              "      <th>BRAF_N</th>\n",
              "      <th>CAMKII_N</th>\n",
              "      <th>CREB_N</th>\n",
              "      <th>ELK_N</th>\n",
              "      <th>ERK_N</th>\n",
              "      <th>GSK3B_N</th>\n",
              "      <th>JNK_N</th>\n",
              "      <th>MEK_N</th>\n",
              "      <th>TRKA_N</th>\n",
              "      <th>RSK_N</th>\n",
              "      <th>APP_N</th>\n",
              "      <th>Bcatenin_N</th>\n",
              "      <th>SOD1_N</th>\n",
              "      <th>MTOR_N</th>\n",
              "      <th>P38_N</th>\n",
              "      <th>pMTOR_N</th>\n",
              "      <th>DSCR1_N</th>\n",
              "      <th>AMPKA_N</th>\n",
              "      <th>NR2B_N</th>\n",
              "      <th>pNUMB_N</th>\n",
              "      <th>...</th>\n",
              "      <th>TIAM1_N</th>\n",
              "      <th>pP70S6_N</th>\n",
              "      <th>NUMB_N</th>\n",
              "      <th>P70S6_N</th>\n",
              "      <th>pGSK3B_N</th>\n",
              "      <th>pPKCG_N</th>\n",
              "      <th>CDK5_N</th>\n",
              "      <th>S6_N</th>\n",
              "      <th>ADARB1_N</th>\n",
              "      <th>AcetylH3K9_N</th>\n",
              "      <th>RRP1_N</th>\n",
              "      <th>BAX_N</th>\n",
              "      <th>ARC_N</th>\n",
              "      <th>ERBB4_N</th>\n",
              "      <th>nNOS_N</th>\n",
              "      <th>Tau_N</th>\n",
              "      <th>GFAP_N</th>\n",
              "      <th>GluR3_N</th>\n",
              "      <th>GluR4_N</th>\n",
              "      <th>IL1B_N</th>\n",
              "      <th>P3525_N</th>\n",
              "      <th>pCASP9_N</th>\n",
              "      <th>PSD95_N</th>\n",
              "      <th>SNCA_N</th>\n",
              "      <th>Ubiquitin_N</th>\n",
              "      <th>pGSK3B_Tyr216_N</th>\n",
              "      <th>SHH_N</th>\n",
              "      <th>BAD_N</th>\n",
              "      <th>BCL2_N</th>\n",
              "      <th>pS6_N</th>\n",
              "      <th>pCFOS_N</th>\n",
              "      <th>SYP_N</th>\n",
              "      <th>H3AcK18_N</th>\n",
              "      <th>EGR1_N</th>\n",
              "      <th>H3MeK4_N</th>\n",
              "      <th>CaNA_N</th>\n",
              "      <th>Genotype</th>\n",
              "      <th>Treatment</th>\n",
              "      <th>Behavior</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MouseID</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>309_1</th>\n",
              "      <td>0.503644</td>\n",
              "      <td>0.747193</td>\n",
              "      <td>0.430175</td>\n",
              "      <td>2.816329</td>\n",
              "      <td>5.990152</td>\n",
              "      <td>0.218830</td>\n",
              "      <td>0.177565</td>\n",
              "      <td>2.373744</td>\n",
              "      <td>0.232224</td>\n",
              "      <td>1.750936</td>\n",
              "      <td>0.687906</td>\n",
              "      <td>0.306382</td>\n",
              "      <td>0.402698</td>\n",
              "      <td>0.296927</td>\n",
              "      <td>1.022060</td>\n",
              "      <td>0.605673</td>\n",
              "      <td>1.877684</td>\n",
              "      <td>2.308745</td>\n",
              "      <td>0.441599</td>\n",
              "      <td>0.859366</td>\n",
              "      <td>0.416289</td>\n",
              "      <td>0.369608</td>\n",
              "      <td>0.178944</td>\n",
              "      <td>1.866358</td>\n",
              "      <td>3.685247</td>\n",
              "      <td>1.537227</td>\n",
              "      <td>0.264526</td>\n",
              "      <td>0.319677</td>\n",
              "      <td>0.813866</td>\n",
              "      <td>0.165846</td>\n",
              "      <td>0.453910</td>\n",
              "      <td>3.037621</td>\n",
              "      <td>0.369510</td>\n",
              "      <td>0.458539</td>\n",
              "      <td>0.335336</td>\n",
              "      <td>0.825192</td>\n",
              "      <td>0.576916</td>\n",
              "      <td>0.448099</td>\n",
              "      <td>0.586271</td>\n",
              "      <td>0.394721</td>\n",
              "      <td>...</td>\n",
              "      <td>0.482864</td>\n",
              "      <td>0.294170</td>\n",
              "      <td>0.182150</td>\n",
              "      <td>0.842725</td>\n",
              "      <td>0.192608</td>\n",
              "      <td>1.443091</td>\n",
              "      <td>0.294700</td>\n",
              "      <td>0.354605</td>\n",
              "      <td>1.339070</td>\n",
              "      <td>0.170119</td>\n",
              "      <td>0.159102</td>\n",
              "      <td>0.188852</td>\n",
              "      <td>0.106305</td>\n",
              "      <td>0.144989</td>\n",
              "      <td>0.176668</td>\n",
              "      <td>0.125190</td>\n",
              "      <td>0.115291</td>\n",
              "      <td>0.228043</td>\n",
              "      <td>0.142756</td>\n",
              "      <td>0.430957</td>\n",
              "      <td>0.247538</td>\n",
              "      <td>1.603310</td>\n",
              "      <td>2.014875</td>\n",
              "      <td>0.108234</td>\n",
              "      <td>1.044979</td>\n",
              "      <td>0.831557</td>\n",
              "      <td>0.188852</td>\n",
              "      <td>0.122652</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.106305</td>\n",
              "      <td>0.108336</td>\n",
              "      <td>0.427099</td>\n",
              "      <td>0.114783</td>\n",
              "      <td>0.131790</td>\n",
              "      <td>0.128186</td>\n",
              "      <td>1.675652</td>\n",
              "      <td>Control</td>\n",
              "      <td>Memantine</td>\n",
              "      <td>C/S</td>\n",
              "      <td>c-CS-m</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>309_2</th>\n",
              "      <td>0.514617</td>\n",
              "      <td>0.689064</td>\n",
              "      <td>0.411770</td>\n",
              "      <td>2.789514</td>\n",
              "      <td>5.685038</td>\n",
              "      <td>0.211636</td>\n",
              "      <td>0.172817</td>\n",
              "      <td>2.292150</td>\n",
              "      <td>0.226972</td>\n",
              "      <td>1.596377</td>\n",
              "      <td>0.695006</td>\n",
              "      <td>0.299051</td>\n",
              "      <td>0.385987</td>\n",
              "      <td>0.281319</td>\n",
              "      <td>0.956676</td>\n",
              "      <td>0.587559</td>\n",
              "      <td>1.725774</td>\n",
              "      <td>2.043037</td>\n",
              "      <td>0.445222</td>\n",
              "      <td>0.834659</td>\n",
              "      <td>0.400364</td>\n",
              "      <td>0.356178</td>\n",
              "      <td>0.173680</td>\n",
              "      <td>1.761047</td>\n",
              "      <td>3.485287</td>\n",
              "      <td>1.509249</td>\n",
              "      <td>0.255727</td>\n",
              "      <td>0.304419</td>\n",
              "      <td>0.780504</td>\n",
              "      <td>0.157194</td>\n",
              "      <td>0.430940</td>\n",
              "      <td>2.921882</td>\n",
              "      <td>0.342279</td>\n",
              "      <td>0.423560</td>\n",
              "      <td>0.324835</td>\n",
              "      <td>0.761718</td>\n",
              "      <td>0.545097</td>\n",
              "      <td>0.420876</td>\n",
              "      <td>0.545097</td>\n",
              "      <td>0.368255</td>\n",
              "      <td>...</td>\n",
              "      <td>0.454519</td>\n",
              "      <td>0.276431</td>\n",
              "      <td>0.182086</td>\n",
              "      <td>0.847615</td>\n",
              "      <td>0.194815</td>\n",
              "      <td>1.439460</td>\n",
              "      <td>0.294060</td>\n",
              "      <td>0.354548</td>\n",
              "      <td>1.306323</td>\n",
              "      <td>0.171427</td>\n",
              "      <td>0.158129</td>\n",
              "      <td>0.184570</td>\n",
              "      <td>0.106592</td>\n",
              "      <td>0.150471</td>\n",
              "      <td>0.178309</td>\n",
              "      <td>0.134275</td>\n",
              "      <td>0.118235</td>\n",
              "      <td>0.238073</td>\n",
              "      <td>0.142037</td>\n",
              "      <td>0.457156</td>\n",
              "      <td>0.257632</td>\n",
              "      <td>1.671738</td>\n",
              "      <td>2.004605</td>\n",
              "      <td>0.109749</td>\n",
              "      <td>1.009883</td>\n",
              "      <td>0.849270</td>\n",
              "      <td>0.200404</td>\n",
              "      <td>0.116682</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.106592</td>\n",
              "      <td>0.104315</td>\n",
              "      <td>0.441581</td>\n",
              "      <td>0.111974</td>\n",
              "      <td>0.135103</td>\n",
              "      <td>0.131119</td>\n",
              "      <td>1.743610</td>\n",
              "      <td>Control</td>\n",
              "      <td>Memantine</td>\n",
              "      <td>C/S</td>\n",
              "      <td>c-CS-m</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>309_3</th>\n",
              "      <td>0.509183</td>\n",
              "      <td>0.730247</td>\n",
              "      <td>0.418309</td>\n",
              "      <td>2.687201</td>\n",
              "      <td>5.622059</td>\n",
              "      <td>0.209011</td>\n",
              "      <td>0.175722</td>\n",
              "      <td>2.283337</td>\n",
              "      <td>0.230247</td>\n",
              "      <td>1.561316</td>\n",
              "      <td>0.677348</td>\n",
              "      <td>0.291276</td>\n",
              "      <td>0.381002</td>\n",
              "      <td>0.281710</td>\n",
              "      <td>1.003635</td>\n",
              "      <td>0.602449</td>\n",
              "      <td>1.731873</td>\n",
              "      <td>2.017984</td>\n",
              "      <td>0.467668</td>\n",
              "      <td>0.814329</td>\n",
              "      <td>0.399847</td>\n",
              "      <td>0.368089</td>\n",
              "      <td>0.173905</td>\n",
              "      <td>1.765544</td>\n",
              "      <td>3.571456</td>\n",
              "      <td>1.501244</td>\n",
              "      <td>0.259614</td>\n",
              "      <td>0.311747</td>\n",
              "      <td>0.785154</td>\n",
              "      <td>0.160895</td>\n",
              "      <td>0.423187</td>\n",
              "      <td>2.944136</td>\n",
              "      <td>0.343696</td>\n",
              "      <td>0.425005</td>\n",
              "      <td>0.324852</td>\n",
              "      <td>0.757031</td>\n",
              "      <td>0.543620</td>\n",
              "      <td>0.404630</td>\n",
              "      <td>0.552994</td>\n",
              "      <td>0.363880</td>\n",
              "      <td>...</td>\n",
              "      <td>0.447197</td>\n",
              "      <td>0.256648</td>\n",
              "      <td>0.184388</td>\n",
              "      <td>0.856166</td>\n",
              "      <td>0.200737</td>\n",
              "      <td>1.524364</td>\n",
              "      <td>0.301881</td>\n",
              "      <td>0.386087</td>\n",
              "      <td>1.279600</td>\n",
              "      <td>0.185456</td>\n",
              "      <td>0.148696</td>\n",
              "      <td>0.190532</td>\n",
              "      <td>0.108303</td>\n",
              "      <td>0.145330</td>\n",
              "      <td>0.176213</td>\n",
              "      <td>0.132560</td>\n",
              "      <td>0.117760</td>\n",
              "      <td>0.244817</td>\n",
              "      <td>0.142445</td>\n",
              "      <td>0.510472</td>\n",
              "      <td>0.255343</td>\n",
              "      <td>1.663550</td>\n",
              "      <td>2.016831</td>\n",
              "      <td>0.108196</td>\n",
              "      <td>0.996848</td>\n",
              "      <td>0.846709</td>\n",
              "      <td>0.193685</td>\n",
              "      <td>0.118508</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.108303</td>\n",
              "      <td>0.106219</td>\n",
              "      <td>0.435777</td>\n",
              "      <td>0.111883</td>\n",
              "      <td>0.133362</td>\n",
              "      <td>0.127431</td>\n",
              "      <td>1.926427</td>\n",
              "      <td>Control</td>\n",
              "      <td>Memantine</td>\n",
              "      <td>C/S</td>\n",
              "      <td>c-CS-m</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>309_4</th>\n",
              "      <td>0.442107</td>\n",
              "      <td>0.617076</td>\n",
              "      <td>0.358626</td>\n",
              "      <td>2.466947</td>\n",
              "      <td>4.979503</td>\n",
              "      <td>0.222886</td>\n",
              "      <td>0.176463</td>\n",
              "      <td>2.152301</td>\n",
              "      <td>0.207004</td>\n",
              "      <td>1.595086</td>\n",
              "      <td>0.583277</td>\n",
              "      <td>0.296729</td>\n",
              "      <td>0.377087</td>\n",
              "      <td>0.313832</td>\n",
              "      <td>0.875390</td>\n",
              "      <td>0.520293</td>\n",
              "      <td>1.566852</td>\n",
              "      <td>2.132754</td>\n",
              "      <td>0.477671</td>\n",
              "      <td>0.727705</td>\n",
              "      <td>0.385639</td>\n",
              "      <td>0.362970</td>\n",
              "      <td>0.179449</td>\n",
              "      <td>1.286277</td>\n",
              "      <td>2.970137</td>\n",
              "      <td>1.419710</td>\n",
              "      <td>0.259536</td>\n",
              "      <td>0.279218</td>\n",
              "      <td>0.734492</td>\n",
              "      <td>0.162210</td>\n",
              "      <td>0.410615</td>\n",
              "      <td>2.500204</td>\n",
              "      <td>0.344509</td>\n",
              "      <td>0.429211</td>\n",
              "      <td>0.330121</td>\n",
              "      <td>0.746980</td>\n",
              "      <td>0.546763</td>\n",
              "      <td>0.386860</td>\n",
              "      <td>0.547849</td>\n",
              "      <td>0.366771</td>\n",
              "      <td>...</td>\n",
              "      <td>0.442650</td>\n",
              "      <td>0.398534</td>\n",
              "      <td>0.161768</td>\n",
              "      <td>0.760234</td>\n",
              "      <td>0.184169</td>\n",
              "      <td>1.612382</td>\n",
              "      <td>0.296382</td>\n",
              "      <td>0.290680</td>\n",
              "      <td>1.198765</td>\n",
              "      <td>0.159799</td>\n",
              "      <td>0.166112</td>\n",
              "      <td>0.185323</td>\n",
              "      <td>0.103184</td>\n",
              "      <td>0.140656</td>\n",
              "      <td>0.163804</td>\n",
              "      <td>0.123210</td>\n",
              "      <td>0.117439</td>\n",
              "      <td>0.234947</td>\n",
              "      <td>0.145068</td>\n",
              "      <td>0.430996</td>\n",
              "      <td>0.251103</td>\n",
              "      <td>1.484624</td>\n",
              "      <td>1.957233</td>\n",
              "      <td>0.119883</td>\n",
              "      <td>0.990225</td>\n",
              "      <td>0.833277</td>\n",
              "      <td>0.192112</td>\n",
              "      <td>0.132781</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.103184</td>\n",
              "      <td>0.111262</td>\n",
              "      <td>0.391691</td>\n",
              "      <td>0.130405</td>\n",
              "      <td>0.147444</td>\n",
              "      <td>0.146901</td>\n",
              "      <td>1.700563</td>\n",
              "      <td>Control</td>\n",
              "      <td>Memantine</td>\n",
              "      <td>C/S</td>\n",
              "      <td>c-CS-m</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>309_5</th>\n",
              "      <td>0.434940</td>\n",
              "      <td>0.617430</td>\n",
              "      <td>0.358802</td>\n",
              "      <td>2.365785</td>\n",
              "      <td>4.718679</td>\n",
              "      <td>0.213106</td>\n",
              "      <td>0.173627</td>\n",
              "      <td>2.134014</td>\n",
              "      <td>0.192158</td>\n",
              "      <td>1.504230</td>\n",
              "      <td>0.550960</td>\n",
              "      <td>0.286961</td>\n",
              "      <td>0.363502</td>\n",
              "      <td>0.277964</td>\n",
              "      <td>0.864912</td>\n",
              "      <td>0.507990</td>\n",
              "      <td>1.480059</td>\n",
              "      <td>2.013697</td>\n",
              "      <td>0.483416</td>\n",
              "      <td>0.687794</td>\n",
              "      <td>0.367531</td>\n",
              "      <td>0.355311</td>\n",
              "      <td>0.174836</td>\n",
              "      <td>1.324695</td>\n",
              "      <td>2.896334</td>\n",
              "      <td>1.359876</td>\n",
              "      <td>0.250705</td>\n",
              "      <td>0.273667</td>\n",
              "      <td>0.702699</td>\n",
              "      <td>0.154827</td>\n",
              "      <td>0.398550</td>\n",
              "      <td>2.456560</td>\n",
              "      <td>0.329126</td>\n",
              "      <td>0.408755</td>\n",
              "      <td>0.313415</td>\n",
              "      <td>0.691956</td>\n",
              "      <td>0.536860</td>\n",
              "      <td>0.360816</td>\n",
              "      <td>0.512824</td>\n",
              "      <td>0.351551</td>\n",
              "      <td>...</td>\n",
              "      <td>0.419095</td>\n",
              "      <td>0.393447</td>\n",
              "      <td>0.160200</td>\n",
              "      <td>0.768113</td>\n",
              "      <td>0.185718</td>\n",
              "      <td>1.645807</td>\n",
              "      <td>0.296829</td>\n",
              "      <td>0.309345</td>\n",
              "      <td>1.206995</td>\n",
              "      <td>0.164650</td>\n",
              "      <td>0.160687</td>\n",
              "      <td>0.188221</td>\n",
              "      <td>0.104784</td>\n",
              "      <td>0.141983</td>\n",
              "      <td>0.167710</td>\n",
              "      <td>0.136838</td>\n",
              "      <td>0.116048</td>\n",
              "      <td>0.255528</td>\n",
              "      <td>0.140871</td>\n",
              "      <td>0.481227</td>\n",
              "      <td>0.251773</td>\n",
              "      <td>1.534835</td>\n",
              "      <td>2.009109</td>\n",
              "      <td>0.119524</td>\n",
              "      <td>0.997775</td>\n",
              "      <td>0.878668</td>\n",
              "      <td>0.205604</td>\n",
              "      <td>0.129954</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.104784</td>\n",
              "      <td>0.110694</td>\n",
              "      <td>0.434154</td>\n",
              "      <td>0.118481</td>\n",
              "      <td>0.140314</td>\n",
              "      <td>0.148380</td>\n",
              "      <td>1.839730</td>\n",
              "      <td>Control</td>\n",
              "      <td>Memantine</td>\n",
              "      <td>C/S</td>\n",
              "      <td>c-CS-m</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>J3295_11</th>\n",
              "      <td>0.254860</td>\n",
              "      <td>0.463591</td>\n",
              "      <td>0.254860</td>\n",
              "      <td>2.092082</td>\n",
              "      <td>2.600035</td>\n",
              "      <td>0.211736</td>\n",
              "      <td>0.171262</td>\n",
              "      <td>2.483740</td>\n",
              "      <td>0.207317</td>\n",
              "      <td>1.057971</td>\n",
              "      <td>0.265642</td>\n",
              "      <td>0.294097</td>\n",
              "      <td>0.249912</td>\n",
              "      <td>0.261223</td>\n",
              "      <td>0.746200</td>\n",
              "      <td>0.510604</td>\n",
              "      <td>1.220926</td>\n",
              "      <td>1.241958</td>\n",
              "      <td>0.422764</td>\n",
              "      <td>0.638211</td>\n",
              "      <td>0.255744</td>\n",
              "      <td>0.330859</td>\n",
              "      <td>0.190173</td>\n",
              "      <td>0.896430</td>\n",
              "      <td>1.822906</td>\n",
              "      <td>0.993107</td>\n",
              "      <td>0.236303</td>\n",
              "      <td>0.249205</td>\n",
              "      <td>0.641746</td>\n",
              "      <td>0.165606</td>\n",
              "      <td>0.372216</td>\n",
              "      <td>1.828208</td>\n",
              "      <td>0.820078</td>\n",
              "      <td>0.380170</td>\n",
              "      <td>0.320431</td>\n",
              "      <td>0.599328</td>\n",
              "      <td>0.508130</td>\n",
              "      <td>0.270060</td>\n",
              "      <td>0.437964</td>\n",
              "      <td>0.260516</td>\n",
              "      <td>...</td>\n",
              "      <td>0.340756</td>\n",
              "      <td>0.527041</td>\n",
              "      <td>0.209433</td>\n",
              "      <td>0.787900</td>\n",
              "      <td>0.178130</td>\n",
              "      <td>2.630825</td>\n",
              "      <td>0.319062</td>\n",
              "      <td>0.654548</td>\n",
              "      <td>0.737226</td>\n",
              "      <td>0.532987</td>\n",
              "      <td>0.196659</td>\n",
              "      <td>0.182762</td>\n",
              "      <td>0.115806</td>\n",
              "      <td>0.160303</td>\n",
              "      <td>0.189360</td>\n",
              "      <td>0.411286</td>\n",
              "      <td>0.134896</td>\n",
              "      <td>0.207748</td>\n",
              "      <td>0.134475</td>\n",
              "      <td>0.503650</td>\n",
              "      <td>0.326362</td>\n",
              "      <td>1.323554</td>\n",
              "      <td>2.578046</td>\n",
              "      <td>0.167181</td>\n",
              "      <td>1.261651</td>\n",
              "      <td>0.962942</td>\n",
              "      <td>0.275547</td>\n",
              "      <td>0.190483</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.115806</td>\n",
              "      <td>0.183324</td>\n",
              "      <td>0.374088</td>\n",
              "      <td>0.318782</td>\n",
              "      <td>0.204660</td>\n",
              "      <td>0.328327</td>\n",
              "      <td>1.364823</td>\n",
              "      <td>Ts65Dn</td>\n",
              "      <td>Saline</td>\n",
              "      <td>S/C</td>\n",
              "      <td>t-SC-s</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>J3295_12</th>\n",
              "      <td>0.272198</td>\n",
              "      <td>0.474163</td>\n",
              "      <td>0.251638</td>\n",
              "      <td>2.161390</td>\n",
              "      <td>2.801492</td>\n",
              "      <td>0.251274</td>\n",
              "      <td>0.182496</td>\n",
              "      <td>2.512737</td>\n",
              "      <td>0.216339</td>\n",
              "      <td>1.081150</td>\n",
              "      <td>0.270378</td>\n",
              "      <td>0.285116</td>\n",
              "      <td>0.249818</td>\n",
              "      <td>0.252547</td>\n",
              "      <td>0.749818</td>\n",
              "      <td>0.524381</td>\n",
              "      <td>1.218705</td>\n",
              "      <td>1.361354</td>\n",
              "      <td>0.415211</td>\n",
              "      <td>0.645197</td>\n",
              "      <td>0.252001</td>\n",
              "      <td>0.338610</td>\n",
              "      <td>0.181223</td>\n",
              "      <td>0.958879</td>\n",
              "      <td>1.879913</td>\n",
              "      <td>0.974891</td>\n",
              "      <td>0.245451</td>\n",
              "      <td>0.262191</td>\n",
              "      <td>0.693595</td>\n",
              "      <td>0.191594</td>\n",
              "      <td>0.360990</td>\n",
              "      <td>1.883370</td>\n",
              "      <td>0.854258</td>\n",
              "      <td>0.380277</td>\n",
              "      <td>0.338246</td>\n",
              "      <td>0.614629</td>\n",
              "      <td>0.519105</td>\n",
              "      <td>0.273472</td>\n",
              "      <td>0.580058</td>\n",
              "      <td>0.275837</td>\n",
              "      <td>...</td>\n",
              "      <td>0.346252</td>\n",
              "      <td>0.518377</td>\n",
              "      <td>0.194333</td>\n",
              "      <td>0.763096</td>\n",
              "      <td>0.170422</td>\n",
              "      <td>2.593227</td>\n",
              "      <td>0.318867</td>\n",
              "      <td>0.632066</td>\n",
              "      <td>0.756047</td>\n",
              "      <td>0.546648</td>\n",
              "      <td>0.188390</td>\n",
              "      <td>0.166966</td>\n",
              "      <td>0.113614</td>\n",
              "      <td>0.161576</td>\n",
              "      <td>0.187146</td>\n",
              "      <td>0.402073</td>\n",
              "      <td>0.130615</td>\n",
              "      <td>0.205114</td>\n",
              "      <td>0.122184</td>\n",
              "      <td>0.512647</td>\n",
              "      <td>0.344160</td>\n",
              "      <td>1.275605</td>\n",
              "      <td>2.534347</td>\n",
              "      <td>0.169592</td>\n",
              "      <td>1.254872</td>\n",
              "      <td>0.983690</td>\n",
              "      <td>0.283207</td>\n",
              "      <td>0.190463</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.113614</td>\n",
              "      <td>0.175674</td>\n",
              "      <td>0.375259</td>\n",
              "      <td>0.325639</td>\n",
              "      <td>0.200415</td>\n",
              "      <td>0.293435</td>\n",
              "      <td>1.364478</td>\n",
              "      <td>Ts65Dn</td>\n",
              "      <td>Saline</td>\n",
              "      <td>S/C</td>\n",
              "      <td>t-SC-s</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>J3295_13</th>\n",
              "      <td>0.228700</td>\n",
              "      <td>0.395179</td>\n",
              "      <td>0.234118</td>\n",
              "      <td>1.733184</td>\n",
              "      <td>2.220852</td>\n",
              "      <td>0.220665</td>\n",
              "      <td>0.161435</td>\n",
              "      <td>1.989723</td>\n",
              "      <td>0.185164</td>\n",
              "      <td>0.884342</td>\n",
              "      <td>0.255045</td>\n",
              "      <td>0.245703</td>\n",
              "      <td>0.221413</td>\n",
              "      <td>0.255792</td>\n",
              "      <td>0.636024</td>\n",
              "      <td>0.442638</td>\n",
              "      <td>1.015695</td>\n",
              "      <td>1.065022</td>\n",
              "      <td>0.408259</td>\n",
              "      <td>0.540172</td>\n",
              "      <td>0.238042</td>\n",
              "      <td>0.326981</td>\n",
              "      <td>0.212631</td>\n",
              "      <td>0.762892</td>\n",
              "      <td>1.425262</td>\n",
              "      <td>0.818199</td>\n",
              "      <td>0.216741</td>\n",
              "      <td>0.235426</td>\n",
              "      <td>0.559043</td>\n",
              "      <td>0.168161</td>\n",
              "      <td>0.309978</td>\n",
              "      <td>1.494208</td>\n",
              "      <td>0.661809</td>\n",
              "      <td>0.337444</td>\n",
              "      <td>0.309978</td>\n",
              "      <td>0.510650</td>\n",
              "      <td>0.463378</td>\n",
              "      <td>0.234679</td>\n",
              "      <td>0.400037</td>\n",
              "      <td>0.235239</td>\n",
              "      <td>...</td>\n",
              "      <td>0.292788</td>\n",
              "      <td>0.460202</td>\n",
              "      <td>0.196736</td>\n",
              "      <td>0.804896</td>\n",
              "      <td>0.170807</td>\n",
              "      <td>2.628286</td>\n",
              "      <td>0.313327</td>\n",
              "      <td>0.669810</td>\n",
              "      <td>0.764098</td>\n",
              "      <td>0.536899</td>\n",
              "      <td>0.201269</td>\n",
              "      <td>0.169175</td>\n",
              "      <td>0.118948</td>\n",
              "      <td>0.174252</td>\n",
              "      <td>0.185131</td>\n",
              "      <td>0.395648</td>\n",
              "      <td>0.137081</td>\n",
              "      <td>0.201088</td>\n",
              "      <td>0.126927</td>\n",
              "      <td>0.631188</td>\n",
              "      <td>0.358114</td>\n",
              "      <td>1.437534</td>\n",
              "      <td>2.544515</td>\n",
              "      <td>0.179692</td>\n",
              "      <td>1.242248</td>\n",
              "      <td>0.976609</td>\n",
              "      <td>0.290843</td>\n",
              "      <td>0.216682</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.118948</td>\n",
              "      <td>0.158296</td>\n",
              "      <td>0.422121</td>\n",
              "      <td>0.321306</td>\n",
              "      <td>0.229193</td>\n",
              "      <td>0.355213</td>\n",
              "      <td>1.430825</td>\n",
              "      <td>Ts65Dn</td>\n",
              "      <td>Saline</td>\n",
              "      <td>S/C</td>\n",
              "      <td>t-SC-s</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>J3295_14</th>\n",
              "      <td>0.221242</td>\n",
              "      <td>0.412894</td>\n",
              "      <td>0.243974</td>\n",
              "      <td>1.876347</td>\n",
              "      <td>2.384088</td>\n",
              "      <td>0.208897</td>\n",
              "      <td>0.173623</td>\n",
              "      <td>2.086028</td>\n",
              "      <td>0.192044</td>\n",
              "      <td>0.922595</td>\n",
              "      <td>0.230649</td>\n",
              "      <td>0.263179</td>\n",
              "      <td>0.224378</td>\n",
              "      <td>0.277484</td>\n",
              "      <td>0.665099</td>\n",
              "      <td>0.479522</td>\n",
              "      <td>1.077014</td>\n",
              "      <td>1.115030</td>\n",
              "      <td>0.434646</td>\n",
              "      <td>0.564766</td>\n",
              "      <td>0.256712</td>\n",
              "      <td>0.313149</td>\n",
              "      <td>0.172252</td>\n",
              "      <td>0.781893</td>\n",
              "      <td>1.558887</td>\n",
              "      <td>0.872820</td>\n",
              "      <td>0.238095</td>\n",
              "      <td>0.270429</td>\n",
              "      <td>0.585146</td>\n",
              "      <td>0.163433</td>\n",
              "      <td>0.341172</td>\n",
              "      <td>1.571820</td>\n",
              "      <td>0.698413</td>\n",
              "      <td>0.356457</td>\n",
              "      <td>0.308446</td>\n",
              "      <td>0.545757</td>\n",
              "      <td>0.480502</td>\n",
              "      <td>0.258083</td>\n",
              "      <td>0.426612</td>\n",
              "      <td>0.258475</td>\n",
              "      <td>...</td>\n",
              "      <td>0.301587</td>\n",
              "      <td>0.490496</td>\n",
              "      <td>0.197567</td>\n",
              "      <td>0.784819</td>\n",
              "      <td>0.175413</td>\n",
              "      <td>2.659706</td>\n",
              "      <td>0.341021</td>\n",
              "      <td>0.642637</td>\n",
              "      <td>0.783185</td>\n",
              "      <td>0.538224</td>\n",
              "      <td>0.212094</td>\n",
              "      <td>0.161431</td>\n",
              "      <td>0.125295</td>\n",
              "      <td>0.172508</td>\n",
              "      <td>0.193753</td>\n",
              "      <td>0.414200</td>\n",
              "      <td>0.149265</td>\n",
              "      <td>0.204467</td>\n",
              "      <td>0.124569</td>\n",
              "      <td>0.621754</td>\n",
              "      <td>0.352279</td>\n",
              "      <td>1.498820</td>\n",
              "      <td>2.609769</td>\n",
              "      <td>0.185037</td>\n",
              "      <td>1.301071</td>\n",
              "      <td>0.989286</td>\n",
              "      <td>0.306701</td>\n",
              "      <td>0.222263</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.125295</td>\n",
              "      <td>0.196296</td>\n",
              "      <td>0.397676</td>\n",
              "      <td>0.335936</td>\n",
              "      <td>0.251317</td>\n",
              "      <td>0.365353</td>\n",
              "      <td>1.404031</td>\n",
              "      <td>Ts65Dn</td>\n",
              "      <td>Saline</td>\n",
              "      <td>S/C</td>\n",
              "      <td>t-SC-s</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>J3295_15</th>\n",
              "      <td>0.302626</td>\n",
              "      <td>0.461059</td>\n",
              "      <td>0.256564</td>\n",
              "      <td>2.092790</td>\n",
              "      <td>2.594348</td>\n",
              "      <td>0.251001</td>\n",
              "      <td>0.191811</td>\n",
              "      <td>2.361816</td>\n",
              "      <td>0.223632</td>\n",
              "      <td>1.064085</td>\n",
              "      <td>0.276146</td>\n",
              "      <td>0.293725</td>\n",
              "      <td>0.275478</td>\n",
              "      <td>0.315754</td>\n",
              "      <td>0.713173</td>\n",
              "      <td>0.502225</td>\n",
              "      <td>1.231865</td>\n",
              "      <td>1.282377</td>\n",
              "      <td>0.429239</td>\n",
              "      <td>0.629506</td>\n",
              "      <td>0.311972</td>\n",
              "      <td>0.349800</td>\n",
              "      <td>0.187583</td>\n",
              "      <td>0.884735</td>\n",
              "      <td>1.785937</td>\n",
              "      <td>0.970182</td>\n",
              "      <td>0.287049</td>\n",
              "      <td>0.281709</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.190031</td>\n",
              "      <td>0.402982</td>\n",
              "      <td>1.742768</td>\n",
              "      <td>0.786827</td>\n",
              "      <td>0.399866</td>\n",
              "      <td>0.339119</td>\n",
              "      <td>0.602804</td>\n",
              "      <td>0.533823</td>\n",
              "      <td>0.287717</td>\n",
              "      <td>0.637962</td>\n",
              "      <td>0.281486</td>\n",
              "      <td>...</td>\n",
              "      <td>0.368269</td>\n",
              "      <td>0.546729</td>\n",
              "      <td>0.188807</td>\n",
              "      <td>0.772752</td>\n",
              "      <td>0.172716</td>\n",
              "      <td>2.654926</td>\n",
              "      <td>0.317003</td>\n",
              "      <td>0.631682</td>\n",
              "      <td>0.736635</td>\n",
              "      <td>0.535312</td>\n",
              "      <td>0.193992</td>\n",
              "      <td>0.172180</td>\n",
              "      <td>0.118899</td>\n",
              "      <td>0.172001</td>\n",
              "      <td>0.191311</td>\n",
              "      <td>0.393170</td>\n",
              "      <td>0.140533</td>\n",
              "      <td>0.204363</td>\n",
              "      <td>0.121402</td>\n",
              "      <td>0.630252</td>\n",
              "      <td>0.356517</td>\n",
              "      <td>1.490077</td>\n",
              "      <td>2.526372</td>\n",
              "      <td>0.184516</td>\n",
              "      <td>1.267120</td>\n",
              "      <td>1.020383</td>\n",
              "      <td>0.292330</td>\n",
              "      <td>0.227606</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.118899</td>\n",
              "      <td>0.187556</td>\n",
              "      <td>0.420347</td>\n",
              "      <td>0.335062</td>\n",
              "      <td>0.252995</td>\n",
              "      <td>0.365278</td>\n",
              "      <td>1.370999</td>\n",
              "      <td>Ts65Dn</td>\n",
              "      <td>Saline</td>\n",
              "      <td>S/C</td>\n",
              "      <td>t-SC-s</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1080 rows  81 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          DYRK1A_N   ITSN1_N    BDNF_N  ...  Treatment  Behavior   class\n",
              "MouseID                                 ...                             \n",
              "309_1     0.503644  0.747193  0.430175  ...  Memantine       C/S  c-CS-m\n",
              "309_2     0.514617  0.689064  0.411770  ...  Memantine       C/S  c-CS-m\n",
              "309_3     0.509183  0.730247  0.418309  ...  Memantine       C/S  c-CS-m\n",
              "309_4     0.442107  0.617076  0.358626  ...  Memantine       C/S  c-CS-m\n",
              "309_5     0.434940  0.617430  0.358802  ...  Memantine       C/S  c-CS-m\n",
              "...            ...       ...       ...  ...        ...       ...     ...\n",
              "J3295_11  0.254860  0.463591  0.254860  ...     Saline       S/C  t-SC-s\n",
              "J3295_12  0.272198  0.474163  0.251638  ...     Saline       S/C  t-SC-s\n",
              "J3295_13  0.228700  0.395179  0.234118  ...     Saline       S/C  t-SC-s\n",
              "J3295_14  0.221242  0.412894  0.243974  ...     Saline       S/C  t-SC-s\n",
              "J3295_15  0.302626  0.461059  0.256564  ...     Saline       S/C  t-SC-s\n",
              "\n",
              "[1080 rows x 81 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgQxBMWbZObv"
      },
      "source": [
        "This data has missing values.  The site:\n",
        "\n",
        "http://pandas.pydata.org/pandas-docs/stable/missing_data.html\n",
        "\n",
        "has an excellent summary of methods to deal with missing values.  Following the techniques there, create a new data frame `df1` where the missing values in each column are filled with the mean values from the non-missing values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVCsYyeIZObv"
      },
      "source": [
        "# TODO 2\n",
        "df1 = df.fillna(df.mean())"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvmtylyvZObw"
      },
      "source": [
        "## Binary Classification for Down's Syndrome\n",
        "\n",
        "We will first predict the binary class label in `df1['Genotype']` which indicates if the mouse has Down's syndrome or not.  Get the string values in `df1['Genotype'].values` and convert this to a numeric vector `y` with 0 or 1.  You may wish to use the `np.unique` command with the `return_inverse=True` option."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oy4TWQHwZObw"
      },
      "source": [
        "# TODO 3\n",
        "y = np.array(np.unique(df1['Genotype'], return_inverse=True)[1])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99--MoN4ZObx"
      },
      "source": [
        "As predictors, get all but the last four columns of the dataframes.  Store the data matrix into `X` and the names of the columns in `xnames`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nv-ojV-jZObx"
      },
      "source": [
        "# TODO 4\n",
        "xnames = df1.columns[:-4]\n",
        "X = df1[xnames].to_numpy()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8u5omy0eZOby"
      },
      "source": [
        "Split the data into training and test with 30% allocated for test.  You can use the train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C53JFqysZOby"
      },
      "source": [
        "# TODO 5\n",
        "#Use : shuffle=True, random_state=3 so we all can have same split.\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "Xtr, Xts, ytr, yts = train_test_split(X, y, test_size=0.33, shuffle=True, random_state=3)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9l7X0pzWZOby"
      },
      "source": [
        "Scale the data with the `StandardScaler`.  Store the scaled values in `Xtr1` and `Xts1`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijwZ4ci8ZObz"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# TODO 6\n",
        "xscal = StandardScaler()\n",
        "yscal = StandardScaler()\n",
        "\n",
        "Xtr1 = xscal.fit_transform(Xtr)\n",
        "Xts1 = xscal.transform(Xts)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvmO5PkcZObz"
      },
      "source": [
        "Create a `LogisticRegression` object `logreg` and `fit` on the scaled training data.  Set the regularization level to `C=1e5` and use the optimizer `solver=liblinear`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GeWMsC2OZObz",
        "outputId": "8857a9df-7ab9-4087-aa3f-911d99f83025"
      },
      "source": [
        "# TODO 7\n",
        "logreg = linear_model.LogisticRegression(C=1e5, solver='liblinear')\n",
        "logreg.fit(Xtr, ytr)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=100000.0, class_weight=None, dual=False,\n",
              "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
              "                   max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1m9zfUWZZObz"
      },
      "source": [
        "Measure the accuracy of the classifer on test data.  You should get around 94%.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwPzTWcxZOb0",
        "outputId": "f0be4065-5f86-4c4a-c682-2a1e17091614"
      },
      "source": [
        "# TODO 8\n",
        "yhat = logreg.predict(Xts)\n",
        "acc = np.mean(yts == yhat)\n",
        "print('Accuracy: {}%'.format((acc*100).round(2)))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 95.8%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yY6wUKiZOb0"
      },
      "source": [
        "## Interpreting the weight vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_MLlS-sZOb0"
      },
      "source": [
        "Create a stem plot of the coefficients, `W` in the logistic regression model.  Jse the `plt.stem()` function with the `use_line_collection=True` option.  You can get the coefficients from `logreg.coef_`, but you will need to reshape this to a 1D array.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "eXx2CepSZOb0",
        "outputId": "ff799029-332a-4578-df26-337720651cf6"
      },
      "source": [
        "# TODO 9\n",
        "W = logreg.coef_.ravel()\n",
        "plt.stem(W, use_line_collection=True)\n",
        "plt.xlabel('coefficient')\n",
        "plt.ylabel('magnitude')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'magnitude')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20KkA7aPZOb1"
      },
      "source": [
        "You should see that `W[i]` is very large for a few components `i`.  These are the genes that are likely to be most involved in Down's Syndrome.   Below we will use L1 regression to enforce sparsity.  Find the names of the genes for two components `i` where the magnitude of `W[i]` is largest.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jb2jm9PuZOb1",
        "outputId": "bad80ad3-a40d-48c5-fa42-9144a76e80db"
      },
      "source": [
        "# TODO 10\n",
        "feats = [np.flipud(np.argsort(np.abs(W)))[0], np.flipud(np.argsort(np.abs(W)))[1]]\n",
        "print('''\n",
        "Feature 1: {}\n",
        "Feature 2: {}\n",
        "'''.format(df1.columns[feats[0]], df1.columns[feats[1]]))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Feature 1: TIAM1_N\n",
            "Feature 2: ITSN1_N\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5m19RtDZOb1"
      },
      "source": [
        "## Cross Validation\n",
        "\n",
        "To obtain a slightly more accurate result, now perform 10-fold cross validation and measure the average precision, recall and f1-score.  Note, that in performing the cross-validation, you will want to randomly permute the test and training sets using the `shuffle` option.  In this data set, all the samples from each class are bunched together, so shuffling is essential.  Print the mean precision, recall and f1-score and error rate across all the folds."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7nkzLWbZOb1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01bbba23-f025-49cd-d3b4-19fccbb3dd3a"
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "nfold = 10\n",
        "kf = KFold(n_splits=nfold,shuffle=True)\n",
        "\n",
        "# TODO 11\n",
        "acc = np.zeros(nfold)\n",
        "prec = np.zeros(nfold)\n",
        "rec = np.zeros(nfold)\n",
        "f1 = np.zeros(nfold)\n",
        "\n",
        "for i, I in enumerate(kf.split(X)):\n",
        "    \n",
        "    # Get training and test data\n",
        "    train, test = I\n",
        "    Xtr = X[train,:]\n",
        "    ytr = y[train]\n",
        "    Xts = X[test,:]\n",
        "    yts = y[test]\n",
        "    \n",
        "    # Scale the data\n",
        "    scal = StandardScaler()\n",
        "    Xtr1 = scal.fit_transform(Xtr)\n",
        "    Xts1 = scal.transform(Xts)    \n",
        "    \n",
        "    # Fit a model    \n",
        "    logreg.fit(Xtr1, ytr)\n",
        "    \n",
        "    # Predict on test samples and measure accuracy\n",
        "    yhat = logreg.predict(Xts1)\n",
        "    acc[i] = np.mean(yhat == yts)\n",
        "    \n",
        "    # Measure other performance metrics\n",
        "    prec[i],rec[i],f1[i],_  = precision_recall_fscore_support(yts,yhat,average='binary') \n",
        "    \n",
        "\n",
        "# Take average values of the metrics\n",
        "precm = np.mean(prec)\n",
        "recm = np.mean(rec)\n",
        "f1m = np.mean(f1)\n",
        "accm= np.mean(acc)\n",
        "\n",
        "# Compute the standard errors\n",
        "prec_se = np.std(prec)/np.sqrt(nfold-1)\n",
        "rec_se = np.std(rec)/np.sqrt(nfold-1)\n",
        "f1_se = np.std(f1)/np.sqrt(nfold-1)\n",
        "acc_se = np.std(acc)/np.sqrt(nfold-1)\n",
        "\n",
        "print('Precision = {0:.4f}, SE = {1:.4f}'.format(precm,prec_se))\n",
        "print('Recall    = {0:.4f}, SE = {1:.4f}'.format(recm, rec_se))\n",
        "print('f1        = {0:.4f}, SE = {1:.4f}'.format(f1m, f1_se))\n",
        "print('Accuracy  = {0:.4f}, SE = {1:.4f}'.format(accm, acc_se))\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision = 0.9463, SE = 0.0096\n",
            "Recall    = 0.9606, SE = 0.0106\n",
            "f1        = 0.9529, SE = 0.0072\n",
            "Accuracy  = 0.9565, SE = 0.0062\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xZMuIiZZOb1"
      },
      "source": [
        "## Multi-Class Classification\n",
        "\n",
        "Now use the response variable in `df1['class']`.  This has 8 possible classes.  Use the `np.unique` funtion as before to convert this to a vector `y` with values 0 to 7."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbbcZ1fpZOb1"
      },
      "source": [
        "# TODO 12\n",
        "y = np.array(np.unique(df1['class'], return_inverse=True)[1])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dj8GEchEZOb2"
      },
      "source": [
        "Fit a multi-class logistic model by creating a `LogisticRegression` object, `logreg` and then calling the `logreg.fit` method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dDzqjYEZOb2"
      },
      "source": [
        "Now perform 10-fold cross validation, and measure the confusion matrix `C` on the test data in each fold. You can use the `confustion_matrix` method in the `sklearn` package.  Add the confusion matrix counts across all folds and then normalize the rows of the confusion matrix so that they sum to one.  Thus, each element `C[i,j]` will represent the fraction of samples where `yhat==j` given `ytrue==i`.  Print the confusion matrix.  You can use the command\n",
        "\n",
        "    print(np.array_str(C, precision=4, suppress_small=True))\n",
        "    \n",
        "to create a nicely formatted print.  Also print the overall mean and SE of the test accuracy across the folds."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cf29s6OKZOb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fe4f013-996f-48f8-e668-91e62f169eaf"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# TODO 13\n",
        "nfold = 10\n",
        "kf = KFold(n_splits=nfold,shuffle=True)\n",
        "\n",
        "C_matrices = np.empty(nfold, dtype=np.ndarray)\n",
        "acc = np.zeros(nfold)\n",
        "\n",
        "for i, I in enumerate(kf.split(X)):\n",
        "    \n",
        "    # Get training and test data\n",
        "    train, test = I\n",
        "    Xtr = X[train,:]\n",
        "    ytr = y[train]\n",
        "    Xts = X[test,:]\n",
        "    yts = y[test]\n",
        "    \n",
        "    # Scale the data\n",
        "    scal = StandardScaler()\n",
        "    Xtr1 = scal.fit_transform(Xtr)\n",
        "    Xts1 = scal.transform(Xts)    \n",
        "    \n",
        "    # Fit a model    \n",
        "    logreg.fit(Xtr1, ytr)\n",
        "    \n",
        "    # Predict on test samples and measure accuracy\n",
        "    yhat = logreg.predict(Xts1)\n",
        "    acc[i] = np.mean(yhat == yts) \n",
        "    C_matrices[i] = confusion_matrix(yts, yhat)\n",
        "\n",
        "# Confusion Matrix\n",
        "C = np.sum(C_matrices)\n",
        "C = C/C.sum(axis=1)[:, None]\n",
        "\n",
        "# Take average accuracy and standard error\n",
        "accm= np.mean(acc)\n",
        "acc_se = np.std(acc)/np.sqrt(nfold-1)\n",
        "\n",
        "print(np.array_str(C, precision=4, suppress_small=True),'\\n')\n",
        "print('Accuracy  = {0:.4f}, SE = {1:.4f}'.format(accm, acc_se))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.9733 0.02   0.     0.     0.0067 0.     0.     0.    ]\n",
            " [0.0296 0.9481 0.     0.     0.0074 0.0074 0.0074 0.    ]\n",
            " [0.     0.     1.     0.     0.     0.     0.     0.    ]\n",
            " [0.0148 0.     0.     0.9852 0.     0.     0.     0.    ]\n",
            " [0.     0.0074 0.     0.     0.9926 0.     0.     0.    ]\n",
            " [0.     0.     0.     0.     0.     1.     0.     0.    ]\n",
            " [0.     0.     0.     0.     0.     0.     1.     0.    ]\n",
            " [0.     0.     0.     0.     0.     0.     0.     1.    ]] \n",
            "\n",
            "Accuracy  = 0.9870, SE = 0.0042\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xo6H9aJOZOb2"
      },
      "source": [
        "Re-run the logistic regression on the entire training data and get the weight coefficients.  This should be a 8 x 77 matrix.  Create a stem plot of the first row of this matrix to see the coefficients on each of the genes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cofO442lZOb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "c42aba42-8add-415e-9a6a-cf3383bfecd9"
      },
      "source": [
        "# TODO 14\n",
        "logreg = linear_model.LogisticRegression(C=1e5, solver='liblinear')\n",
        "logreg.fit(Xtr, ytr)\n",
        "W = logreg.coef_[0]\n",
        "\n",
        "plt.stem(W, use_line_collection=True)\n",
        "plt.xlabel('first row coefficients')\n",
        "plt.ylabel('magnitude')\n",
        "plt.show()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "NodxmY1xZOb2"
      },
      "source": [
        "## L1-Regularization\n",
        "\n",
        "This section is bonus.\n",
        "\n",
        "In most genetic problems, only a limited number of the tested genes are likely influence any particular attribute.  Hence, we would expect that the weight coefficients in the logistic regression model should be sparse.  That is, they should be zero on any gene that plays no role in the particular attribute of interest.  Genetic analysis commonly imposes sparsity by adding an l1-penalty term.  Read the `sklearn` [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) on the `LogisticRegression` class to see how to set the l1-penalty and the inverse regularization strength, `C`.\n",
        "\n",
        "Using the model selection strategies from the [housing demo](../unit05_lasso/demo2_housing.ipynb), use K-fold cross validation to select an appropriate inverse regularization strength.  \n",
        "* Use 10-fold cross validation \n",
        "* You should select around 20 values of `C`.  It is up to you find a good range.\n",
        "* Make appropriate plots and print out to display your results\n",
        "* How does the accuracy compare to the accuracy achieved without regularization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "eQ8JA-JtZOb3"
      },
      "source": [
        "# TODO 15\n",
        "\n",
        "nfold = 10\n",
        "kf = KFold(n_splits=nfold,shuffle=True)\n",
        "\n",
        "reg_coeffs = np.logspace(-1, 0.5 , num=20)\n",
        "accs = np.zeros((nfold, len(reg_coeffs)))\n",
        "\n",
        "for i, I in enumerate(kf.split(X)):\n",
        "    # Get training and test data\n",
        "    train, test = I\n",
        "    Xtr = X[train,:]\n",
        "    ytr = y[train]\n",
        "    Xts = X[test,:]\n",
        "    yts = y[test]\n",
        "    \n",
        "    # Scale the data\n",
        "    scal = StandardScaler()\n",
        "    Xtr1 = scal.fit_transform(Xtr)\n",
        "    Xts1 = scal.transform(Xts)    \n",
        "    \n",
        "    for j, C in enumerate(reg_coeffs):\n",
        "        logreg = linear_model.LogisticRegression(penalty='l1', C=C, solver='liblinear')\n",
        "        logreg.fit(Xtr1, ytr)\n",
        "        yhat = logreg.predict(Xts1)\n",
        "        acc = np.mean(yhat == yts)\n",
        "        accs[i][j] = acc \n",
        "    \n",
        "accs_mean = np.mean(accs, axis=0)\n",
        "accs_se  = np.std(accs, axis=0) / np.sqrt(nfold-1)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "fjZZf8tnTHFM",
        "outputId": "e06a6aeb-653c-4e3e-9869-ed62bfd8fe21"
      },
      "source": [
        "plt.errorbar(reg_coeffs, accs_mean, yerr=accs_se)\n",
        "plt.xlabel('log10(C)')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "print('Maximum Accuracy = {}%'.format(100 * np.max(accs_mean).round(4)))\n",
        "print('log10(C) = {}'.format(reg_coeffs[np.argmax(accs_mean)].round(4)))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum Accuracy = 99.17%\n",
            "log10(C) = 1.5283\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFD1TBkefXVD"
      },
      "source": [
        "How does the accuracy compare to the accuracy achieved without regularization.\n",
        "\n",
        "Answer: The accuracy is not a huge improvement since without regularization it was already at around 99%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRSHM7qww6cu",
        "outputId": "a3c78290-bae0-4f3d-db43-12d3903caf25"
      },
      "source": [
        "opt_C_loc = np.argmax(accs_mean)\n",
        "\n",
        "# Find the lowest model order below the target\n",
        "accs_tgt = accs_mean[opt_C_loc] - accs_se[opt_C_loc]\n",
        "opt_se_C = reg_coeffs[np.min(np.where(accs_mean >= accs_tgt))]\n",
        "\n",
        "print('Optimal One-SE Inverse Coefficient: {}'.format(opt_se_C))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal One-SE Inverse Coefficient: 1.2742749857031335\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypRjtiuaxYTt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}